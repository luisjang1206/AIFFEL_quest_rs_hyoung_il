{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7166dcf2-37a6-4c22-9c77-e052fa6edd20",
   "metadata": {},
   "source": [
    "#GoingDeeper 멋진 사전 만들기 프로젝트  \n",
    "\n",
    "1. 프로젝트 목표   \n",
    "    1.1목표  \n",
    "        -텍스트 처리와 토크나이저 이해  \n",
    "      \n",
    "    1.2 테스크  \n",
    "        -코퍼스 분석, 전처리, SentencePiece 적용, 토크나이저 구현 및 동작 진행.  \n",
    "        -SentencePiece 토크나이저가 적용된 Text Classifier 모델이 정상적으로 수렴하여 80% 이상의 test accuracy가 확인.  \n",
    "        -SentencePiece 와 다른 토크나이저 혹은 SentencePiece의 다른 옵션의 경우와 비교 분석  \n",
    "\n",
    "\n",
    "  \n",
    "2. Baseline 모델은 선택 - GRU   \n",
    "    선택한 이유는 GRU는  LSTM의 미니멀 버전으로 속도가 빠르다.  \n",
    "    본 프로젝트에서는 RNN 계열 내에서 토크나이저 성능 실험이 핵심으로 요구된다.  \n",
    "    따라서 RNN 계열이면서 빠른 실험이 가능한 GRU를 선택하였다.   \n",
    "  \n",
    "3. EDA 결과  (별도 파일 GD01_EDA.ipynb참고)  \n",
    "    -: '좋은', '없다'와 같은 단어나 'ㅋㅋ', 'ㅠㅠ' 같은 기호들은 긍정 부정에서 모두 사용 빈도가 높은 중의적 표현임. 이런 단어 단독으로 감정을 특정하기 어려울 수 있음  \n",
    "    -'최고', '최악', '아깝다'와 같은 형용사나 'ㅎㅎ'(긍정), 'ㅡㅡ'(부정) 같은 특정 기호들은 매우 명확한 긍정/부정 신호로 보임  \n",
    "    -N-gram 분석시 \"재미도 없고\"(부정)와 \"흠잡을 데 없는\"(긍정)처럼 함께 쓰이는 단어를 통해 긍부정 신호가 명확하게 판단됨  \n",
    "  \n",
    "4. 실험 기록  \n",
    "    1) 베이스 라인 구축 : Sentencepece 토크나이저, 모델 GRU  \n",
    "    2) 토크나이저별 vocab_size 실험  \n",
    "        -SentencePiece 토크나이저 vocab_size 실험 : 10000, 15000, 20000, 30000  \n",
    "        -KoNLPy(Mecab) 토크나이저 vocab_size 실험 : 10000, 15000, 20000, 30000  \n",
    "    3) 중의적 단어 불용어 추가 실험  \n",
    "        -SentencePiece 토크나이저 불용어 추가  \n",
    "        -KoNLPy(Mecab) 토크나이저 불용어 추가  \n",
    "    4) 토크나이저 모델 변경  \n",
    "        -Sentencepiece 토크나이저 모델 변경 bpe → unigram  \n",
    "    5) SentencePiece토크나이저 옵션 변경 실험  \n",
    "        -character_coverage,user_defined_symbols 변경  \n",
    "    6) MAX_LEN 변경 실험 40 → 110  \n",
    "    7) KoNLPy (Mecab) 토크나이저 품사(POS) 기반 단어 필터링 추가 실험  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555866b1-7c9c-4605-8ce5-21d5070842fe",
   "metadata": {},
   "outputs": [],
   "source": [
    " # 코드\n",
    "\n",
    "# ====================================================================================\n",
    "# 1. 라이브러리 Import\n",
    "# ====================================================================================\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- 비교 실험을 위한 토크나이저 ---\n",
    "import sentencepiece as spm\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "# ====================================================================================\n",
    "# 2. 중앙 설정 (Configuration)\n",
    "# ====================================================================================\n",
    "CFG = {\n",
    "    # --- 실험 선택 ---\n",
    "    'TOKENIZER_TYPE': 'SentencePiece', # 'SentencePiece' 또는 'KoNLPy' 선택\n",
    "    'KONLPY_TOKENIZER': 'Mecab',     # TOKENIZER_TYPE이 'KoNLPy'일 경우, 사용할 형태소 분석기\n",
    "    \n",
    "    # --- SentencePiece 하이퍼파라미터 ---\n",
    "    'SP_VOCAB_SIZE': 10000,          # SentencePiece 단어 집합 크기\n",
    "    'SP_MODEL_TYPE': 'bpe',          # 'bpe' 또는 'unigram'\n",
    "    \n",
    "    # --- KoNLPy 하이퍼파라미터 (추가) ---\n",
    "    'KONLPY_VOCAB_SIZE': 10000,      # KoNLPy 사용 시 단어 집합 크기 제한\n",
    "    \n",
    "    # --- 모델 하이퍼파라미터 ---\n",
    "    'EMBEDDING_DIM': 128,\n",
    "    'HIDDEN_DIM': 128,\n",
    "    'N_LAYERS': 1,\n",
    "    'DROPOUT': 0.6,\n",
    "    \n",
    "    # --- 학습 하이퍼파라미터 ---\n",
    "    'EPOCHS': 20,\n",
    "    'LEARNING_RATE': 0.001,\n",
    "    'BATCH_SIZE': 128,\n",
    "    'MAX_LEN': 40,                   # 문장 최대 길이\n",
    "    'PATIENCE': 3,                   # 조기 종료 조건\n",
    "    'WEIGHT_DECAY': 1e-5,            # 가중치 감쇠 (L2 규제)\n",
    "    \n",
    "    # --- 기타 설정 ---\n",
    "    'SEED': 42,\n",
    "    'DEVICE': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'MODEL_SAVE_PATH': 'best_model.pth',\n",
    "}\n",
    "\n",
    "# ====================================================================================\n",
    "# 3. 유틸리티 함수\n",
    "# ====================================================================================\n",
    "def seed_everything(seed):\n",
    "    \"\"\"재현성을 위한 시드 고정 함수\"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# ====================================================================================\n",
    "# 4. 데이터 준비 함수\n",
    "# ====================================================================================\n",
    "def load_and_preprocess_data():\n",
    "    \"\"\"NSMC 데이터를 로드하고 전처리하는 함수\"\"\"\n",
    "    data_path = os.path.join(os.getenv(\"HOME\"), 'work', 'workplace', 'AIFFEL_quest_rs', 'Exploration', 'Quest05', 'sentiment_classification', 'data')\n",
    "    train_data = pd.read_table(os.path.join(data_path, 'ratings_train.txt'))\n",
    "    test_data = pd.read_table(os.path.join(data_path, 'ratings_test.txt'))\n",
    "    \n",
    "    # --- 훈련 데이터 정제 ---\n",
    "    train_data.dropna(subset=['document'], inplace=True)\n",
    "    train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    \n",
    "    # --- 테스트 데이터 정제 ---\n",
    "    test_data.dropna(subset=['document'], inplace=True)\n",
    "    test_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    \n",
    "    # --- 데이터셋 분리 ---\n",
    "    train_set, val_set = train_test_split(train_data, test_size=0.2, random_state=CFG['SEED'], stratify=train_data['label'])\n",
    "    \n",
    "    print(\"✅ 데이터 로드 및 전처리 완료\")\n",
    "    return train_set, val_set, test_data\n",
    "\n",
    "# ====================================================================================\n",
    "# 5. 토크나이저 생성 함수 (KoNLPy 로직 수정)\n",
    "# ====================================================================================\n",
    "def get_tokenizer(cfg, train_df):\n",
    "    \"\"\"CFG에 따라 SentencePiece 또는 KoNLPy 토크나이저와 vocab_size를 반환\"\"\"\n",
    "    \n",
    "    if cfg['TOKENIZER_TYPE'] == 'SentencePiece':\n",
    "        # --- (SentencePiece 로직은 이전과 동일) ---\n",
    "        corpus_path = 'nsmc_corpus.txt'\n",
    "        model_prefix = f'nsmc_{cfg[\"SP_MODEL_TYPE\"]}_{cfg[\"SP_VOCAB_SIZE\"]}'\n",
    "        train_df['document'].to_csv(corpus_path, index=False, header=False)\n",
    "        spm.SentencePieceTrainer.train(\n",
    "            f'--input={corpus_path} --model_prefix={model_prefix} '\n",
    "            f'--vocab_size={cfg[\"SP_VOCAB_SIZE\"]} --model_type={cfg[\"SP_MODEL_TYPE\"]}'\n",
    "        )\n",
    "        processor = spm.SentencePieceProcessor()\n",
    "        processor.load(f'{model_prefix}.model')\n",
    "        vocab_size = processor.get_piece_size()\n",
    "\n",
    "        def tokenize_fn(corpus, max_len):\n",
    "            sequences = []\n",
    "            for sentence in corpus:\n",
    "                ids = processor.encode_as_ids(str(sentence))\n",
    "                ids = ids[:max_len] if len(ids) > max_len else ids + [0] * (max_len - len(ids))\n",
    "                sequences.append(ids)\n",
    "            return torch.tensor(sequences, dtype=torch.long)\n",
    "            \n",
    "        print(f\"✅ SentencePiece 토크나이저 준비 완료 (vocab_size: {vocab_size})\")\n",
    "        return tokenize_fn, vocab_size\n",
    "\n",
    "    elif cfg['TOKENIZER_TYPE'] == 'KoNLPy':\n",
    "        # --- KoNLPy 토크나이저 생성 (vocab_size 제한 로직 추가) ---\n",
    "        if cfg['KONLPY_TOKENIZER'] == 'Mecab':\n",
    "            # tokenizer = Mecab()\n",
    "            # 기존 코드: tokenizer = Mecab()\n",
    "            # 수정할 코드:\n",
    "            tokenizer = Mecab('/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ko-dic')\n",
    "\n",
    "        else:\n",
    "            from konlpy.tag import Okt\n",
    "            tokenizer = Okt()\n",
    "            \n",
    "        # 1. 단어 빈도수 계산\n",
    "        word_counts = {}\n",
    "        for sentence in tqdm(train_df['document'], desc=\"KoNLPy Freq. Counting\"):\n",
    "            tokens = tokenizer.morphs(str(sentence))\n",
    "            for token in tokens:\n",
    "                word_counts[token] = word_counts.get(token, 0) + 1\n",
    "        \n",
    "        # 2. 빈도수 기준으로 상위 단어 선택\n",
    "        # <PAD>, <UNK> 토큰을 위해 (vocab_size - 2)개만 선택\n",
    "        sorted_words = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "        top_words = sorted_words[:cfg['KONLPY_VOCAB_SIZE'] - 2]\n",
    "        \n",
    "        # 3. 최종 단어 사전 구축\n",
    "        word_index = {'<PAD>': 0, '<UNK>': 1}\n",
    "        for word in top_words:\n",
    "            word_index[word] = len(word_index)\n",
    "            \n",
    "        vocab_size = len(word_index)\n",
    "\n",
    "        def tokenize_fn(corpus, max_len):\n",
    "            sequences = []\n",
    "            for sentence in corpus:\n",
    "                tokens = tokenizer.morphs(str(sentence))\n",
    "                ids = [word_index.get(token, 1) for token in tokens] # 사전에 없으면 <UNK> (1)\n",
    "                ids = ids[:max_len] if len(ids) > max_len else ids + [0] * (max_len - len(ids))\n",
    "                sequences.append(ids)\n",
    "            return torch.tensor(sequences, dtype=torch.long)\n",
    "            \n",
    "        print(f\"✅ KoNLPy({cfg['KONLPY_TOKENIZER']}) 토크나이저 준비 완료 (vocab_size: {vocab_size})\")\n",
    "        return tokenize_fn, vocab_size\n",
    "\n",
    "# ====================================================================================\n",
    "# 6. 모델 정의\n",
    "# ====================================================================================\n",
    "class SentimentGRU(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        super(SentimentGRU, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=n_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        gru_out, hidden = self.gru(embedded)\n",
    "        last_output = gru_out[:, -1, :]\n",
    "        last_output = self.dropout(last_output)\n",
    "        output = self.fc(last_output)\n",
    "        return output\n",
    "\n",
    "# ====================================================================================\n",
    "# 7. 학습 및 평가 함수 (출력값 수정)\n",
    "# ====================================================================================\n",
    "def run_experiment(cfg, train_set, val_set, test_set):\n",
    "    \"\"\"하나의 설정(CFG)으로 전체 실험을 실행하는 메인 함수\"\"\"\n",
    "    \n",
    "    # --- 1. 토크나이저 및 데이터로더 준비 ---\n",
    "    tokenize_fn, vocab_size = get_tokenizer(cfg, train_set)\n",
    "    \n",
    "    X_train = tokenize_fn(train_set['document'].tolist(), cfg['MAX_LEN'])\n",
    "    y_train = torch.tensor(train_set['label'].values, dtype=torch.float32)\n",
    "    X_val = tokenize_fn(val_set['document'].tolist(), cfg['MAX_LEN'])\n",
    "    y_val = torch.tensor(val_set['label'].values, dtype=torch.float32)\n",
    "    X_test = tokenize_fn(test_set['document'].tolist(), cfg['MAX_LEN'])\n",
    "    y_test = torch.tensor(test_set['label'].values, dtype=torch.float32)\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=cfg['BATCH_SIZE'], shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=cfg['BATCH_SIZE'], shuffle=False)\n",
    "    test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=cfg['BATCH_SIZE'], shuffle=False)\n",
    "    \n",
    "    # --- 2. 모델, 손실함수, 옵티마이저 정의 ---\n",
    "    model = SentimentGRU(vocab_size, cfg['EMBEDDING_DIM'], cfg['HIDDEN_DIM'], cfg['N_LAYERS'], cfg['DROPOUT']).to(cfg['DEVICE'])\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=cfg['LEARNING_RATE'], weight_decay=cfg['WEIGHT_DECAY'])\n",
    "    \n",
    "    # --- 3. 학습 및 조기 종료 ---\n",
    "    patience_counter = 0\n",
    "    best_loss = np.Inf\n",
    "    \n",
    "    print(\"\\n🚀 모델 학습을 시작합니다...\")\n",
    "    for epoch in range(cfg['EPOCHS']):\n",
    "        # --- 훈련 단계 ---\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0, 0, 0\n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1:02d} [Train]\"):\n",
    "            inputs, labels = inputs.to(cfg['DEVICE']), labels.to(cfg['DEVICE'])\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            preds = torch.sigmoid(outputs.squeeze()) > 0.5\n",
    "            train_correct += (preds == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "\n",
    "        # <<<--- 훈련 결과 계산 --- START ---\n",
    "        # 매 에포크의 훈련이 끝나면 평균 손실과 정확도를 계산합니다.\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_accuracy = train_correct / train_total\n",
    "        # <<<--- 훈련 결과 계산 --- END ---\n",
    "        \n",
    "        # --- 검증 단계 ---\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(cfg['DEVICE']), labels.to(cfg['DEVICE'])\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs.squeeze(), labels)\n",
    "                val_loss += loss.item()\n",
    "                preds = torch.sigmoid(outputs.squeeze()) > 0.5\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = val_correct / val_total\n",
    "\n",
    "        # <<<--- 출력문 수정 --- START ---\n",
    "        # 기존 출력문에 avg_train_loss와 train_accuracy를 추가합니다.\n",
    "        print(f\"Epoch {epoch+1:02d} | \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy*100:.2f}% | \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy*100:.2f}%\")\n",
    "        # <<<--- 출력문 수정 --- END ---\n",
    "        \n",
    "        if avg_val_loss < best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), cfg['MODEL_SAVE_PATH'])\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= cfg['PATIENCE']:\n",
    "            print(f\"🛑 Early stopping triggered after {epoch+1} epochs.\")\n",
    "            break\n",
    "            \n",
    "    # --- 4. 최종 평가 ---\n",
    "    print(f\"\\n🧪 최고 성능 모델('{cfg['MODEL_SAVE_PATH']}')로 최종 평가를 시작합니다...\")\n",
    "    model.load_state_dict(torch.load(cfg['MODEL_SAVE_PATH']))\n",
    "    model.eval()\n",
    "    test_loss, test_correct, test_total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc=\"[Testing]\"):\n",
    "            inputs, labels = inputs.to(cfg['DEVICE']), labels.to(cfg['DEVICE'])\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs.squeeze(), labels)\n",
    "            test_loss += loss.item()\n",
    "            preds = torch.sigmoid(outputs.squeeze()) > 0.5\n",
    "            test_correct += (preds == labels).sum().item()\n",
    "            test_total += labels.size(0)\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    test_accuracy = test_correct / test_total\n",
    "    \n",
    "    print(\"\\n🎉 모델 평가가 완료되었습니다.\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"  - 최종 테스트 손실 (Loss): {avg_test_loss:.4f}\")\n",
    "    print(f\"  - 최종 테스트 정확도 (Accuracy): {test_accuracy*100:.2f}%\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    return {'loss': avg_test_loss, 'accuracy': test_accuracy}\n",
    "\n",
    "# ====================================================================================\n",
    "# 8. 메인 실행 블록\n",
    "# ====================================================================================\n",
    "if __name__ == '__main__':\n",
    "    seed_everything(CFG['SEED'])\n",
    "    train_set, val_set, test_set = load_and_preprocess_data()\n",
    "    \n",
    "    # --- 여기서부터 실험을 실행합니다 ---\n",
    "    # 예시 1: SentencePiece (bpe, vocab_size=10000) 실험\n",
    "    results = run_experiment(CFG, train_set, val_set, test_set)\n",
    "    \n",
    "    # 예시 2: KoNLPy (Mecab) 실험 (CFG 변경 후 실행)\n",
    "    # CFG['TOKENIZER_TYPE'] = 'KoNLPy'\n",
    "    # CFG['KONLPY_TOKENIZER'] = 'Mecab'\n",
    "    # results_mecab = run_experiment(CFG, train_set, val_set, test_set)\n",
    "    \n",
    "    # 예시 3: SentencePiece (unigram, vocab_size=16000) 실험 (CFG 변경 후 실행)\n",
    "    # CFG['TOKENIZER_TYPE'] = 'SentencePiece'\n",
    "    # CFG['SP_MODEL_TYPE'] = 'unigram'\n",
    "    # CFG['SP_VOCAB_SIZE'] = 16000\n",
    "    # results_sp_uni = run_experiment(CFG, train_set, val_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06b96ad-cf73-4de7-9145-e0d869c3fe93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87df8fbe-810b-4f3a-98f6-529e8afc96ba",
   "metadata": {},
   "source": [
    "# 실험 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70304a55-5885-4036-86b8-e1e5ab699c8a",
   "metadata": {},
   "source": [
    "# 토크나이저별 Vocab Size 변경 실험"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9f6b4e-c320-466c-b9de-98ab9400beb4",
   "metadata": {},
   "source": [
    "SentencePiece 토크나이저 vocab_size 실험 : 10000, 15000, 20000, 30000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caaae52-8e67-46ea-9941-6331694a11fd",
   "metadata": {},
   "source": [
    "\n",
    "✅ SentencePiece 토크나이저 준비 완료 (vocab_size: 10000)  \n",
    "Epoch 01 | Train Loss: 0.5399 | Train Acc: 70.49% | Val Loss: 0.3979 | Val Acc: 82.02%  \n",
    "Epoch 02 | Train Loss: 0.3511 | Train Acc: 84.81% | Val Loss: 0.3452 | Val Acc: 85.08%  \n",
    "Epoch 03 | Train Loss: 0.3024 | Train Acc: 87.21% | Val Loss: 0.3384 | Val Acc: 85.29%  \n",
    "Epoch 04 | Train Loss: 0.2738 | Train Acc: 88.66% | Val Loss: 0.3370 | Val Acc: 85.42%  \n",
    "Epoch 05 | Train Loss: 0.2450 | Train Acc: 90.09% | Val Loss: 0.3481 | Val Acc: 85.30%  \n",
    "Epoch 06 | Train Loss: 0.2126 | Train Acc: 91.61% | Val Loss: 0.3843 | Val Acc: 85.02%  \n",
    "Epoch 07 | Train Loss: 0.1756 | Train Acc: 93.37% | Val Loss: 0.3792 | Val Acc: 84.85%  \n",
    "🛑 Early stopping triggered after 7 epochs.  \n",
    "  \n",
    "----------------------------------------  \n",
    "  - 최종 테스트 손실 (Loss): 0.3418  \n",
    "  - 최종 테스트 정확도 (Accuracy): 85.39%  \n",
    "----------------------------------------  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bed8da-fd82-4527-8ca1-89f7b63e5a33",
   "metadata": {},
   "source": [
    "✅ SentencePiece 토크나이저 준비 완료 (vocab_size: 15000)  \n",
    "  \n",
    "Epoch 01 | Train Loss: 0.5547 | Train Acc: 69.23% | Val Loss: 0.4035 | Val Acc: 81.66%  \n",
    "Epoch 02 | Train Loss: 0.3527 | Train Acc: 84.83% | Val Loss: 0.3480 | Val Acc: 84.75%  \n",
    "Epoch 03 | Train Loss: 0.2962 | Train Acc: 87.69% | Val Loss: 0.3413 | Val Acc: 85.08%  \n",
    "Epoch 04 | Train Loss: 0.2642 | Train Acc: 89.23% | Val Loss: 0.3485 | Val Acc: 85.34%  \n",
    "Epoch 05 | Train Loss: 0.2351 | Train Acc: 90.60% | Val Loss: 0.3576 | Val Acc: 85.20%  \n",
    "Epoch 06 | Train Loss: 0.2016 | Train Acc: 92.17% | Val Loss: 0.3634 | Val Acc: 84.92%  \n",
    "🛑 Early stopping triggered after 6 epochs. \n",
    " \n",
    "----------------------------------------  \n",
    "  - 최종 테스트 손실 (Loss): 0.3462  \n",
    "  - 최종 테스트 정확도 (Accuracy): 85.05%  \n",
    "----------------------------------------  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1437d57-eb08-407e-8d17-b33350342fa1",
   "metadata": {},
   "source": [
    "✅ SentencePiece 토크나이저 준비 완료 (vocab_size: 20000)  \n",
    "  \n",
    "Epoch 01 | Train Loss: 0.5622 | Train Acc: 68.04% | Val Loss: 0.4033 | Val Acc: 81.60%  \n",
    "Epoch 02 | Train Loss: 0.3500 | Train Acc: 84.83% | Val Loss: 0.3519 | Val Acc: 84.70%  \n",
    "Epoch 03 | Train Loss: 0.2857 | Train Acc: 88.20% | Val Loss: 0.3471 | Val Acc: 85.05%  \n",
    "Epoch 04 | Train Loss: 0.2491 | Train Acc: 89.90% | Val Loss: 0.3487 | Val Acc: 84.98%  \n",
    "Epoch 05 | Train Loss: 0.2156 | Train Acc: 91.57% | Val Loss: 0.3740 | Val Acc: 84.95%  \n",
    "Epoch 06 | Train Loss: 0.1789 | Train Acc: 93.26% | Val Loss: 0.4074 | Val Acc: 84.33%  \n",
    "🛑 Early stopping triggered after 6 epochs.  \n",
    "  \n",
    "----------------------------------------  \n",
    "  - 최종 테스트 손실 (Loss): 0.3494  \n",
    "  - 최종 테스트 정확도 (Accuracy): 85.30%  \n",
    "----------------------------------------  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7592474-8fea-411e-8646-17c20e792ff8",
   "metadata": {},
   "source": [
    "✅ SentencePiece 토크나이저 준비 완료 (vocab_size: 30000)  \n",
    "  \n",
    "Epoch 01 | Train Loss: 0.5570 | Train Acc: 68.91% | Val Loss: 0.4129 | Val Acc: 80.99%  \n",
    "Epoch 02 | Train Loss: 0.3556 | Train Acc: 84.54% | Val Loss: 0.3513 | Val Acc: 84.43%  \n",
    "Epoch 03 | Train Loss: 0.2826 | Train Acc: 88.47% | Val Loss: 0.3419 | Val Acc: 84.96%  \n",
    "Epoch 04 | Train Loss: 0.2386 | Train Acc: 90.62% | Val Loss: 0.3507 | Val Acc: 85.07%  \n",
    "Epoch 05 | Train Loss: 0.1988 | Train Acc: 92.53% | Val Loss: 0.3736 | Val Acc: 84.61%  \n",
    "Epoch 06 | Train Loss: 0.1572 | Train Acc: 94.30% | Val Loss: 0.4311 | Val Acc: 84.42%  \n",
    "🛑 Early stopping triggered after 6 epochs.  \n",
    "  \n",
    "----------------------------------------  \n",
    "  - 최종 테스트 손실 (Loss): 0.3475  \n",
    "  - 최종 테스트 정확도 (Accuracy): 84.92%  \n",
    "----------------------------------------  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce477ba-87c6-4771-88dd-2f2556e6930d",
   "metadata": {},
   "source": [
    "KoNLPy(Mecab) 토크나이저 vocab_size 실험 : 10000, 15000, 20000, 30000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee74eff4-9d96-4151-9bd9-1a38f1afc31f",
   "metadata": {},
   "source": [
    "✅ KoNLPy(Mecab) 토크나이저 준비 완료 (vocab_size: 10000)  \n",
    "  \n",
    "Epoch 01 | Train Loss: 0.4789 | Train Acc: 74.98% | Val Loss: 0.3603 | Val Acc: 83.92%  \n",
    "Epoch 02 | Train Loss: 0.3332 | Train Acc: 85.58% | Val Loss: 0.3225 | Val Acc: 85.93%  \n",
    "Epoch 03 | Train Loss: 0.2912 | Train Acc: 87.72% | Val Loss: 0.3114 | Val Acc: 86.64%  \n",
    "Epoch 04 | Train Loss: 0.2624 | Train Acc: 89.24% | Val Loss: 0.3087 | Val Acc: 86.55%  \n",
    "Epoch 05 | Train Loss: 0.2366 | Train Acc: 90.53% | Val Loss: 0.3193 | Val Acc: 86.60%  \n",
    "Epoch 06 | Train Loss: 0.2090 | Train Acc: 91.77% | Val Loss: 0.3393 | Val Acc: 86.44%  \n",
    "Epoch 07 | Train Loss: 0.1815 | Train Acc: 93.07% | Val Loss: 0.3482 | Val Acc: 86.04%  \n",
    "🛑 Early stopping triggered after 7 epochs.  \n",
    "  \n",
    "----------------------------------------  \n",
    "  - 최종 테스트 손실 (Loss): 0.3182  \n",
    "  - 최종 테스트 정확도 (Accuracy): 86.09%  \n",
    "----------------------------------------  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d8b543-bc08-44c2-a31a-a70a13a2dc1d",
   "metadata": {},
   "source": [
    "✅ KoNLPy(Mecab) 토크나이저 준비 완료 (vocab_size: 15000)  \n",
    "  \n",
    "Epoch 01 | Train Loss: 0.5170 | Train Acc: 71.30% | Val Loss: 0.3669 | Val Acc: 83.51%  \n",
    "Epoch 02 | Train Loss: 0.3343 | Train Acc: 85.60% | Val Loss: 0.3284 | Val Acc: 85.68%  \n",
    "Epoch 03 | Train Loss: 0.2896 | Train Acc: 87.92% | Val Loss: 0.3209 | Val Acc: 86.01%  \n",
    "Epoch 04 | Train Loss: 0.2581 | Train Acc: 89.50% | Val Loss: 0.3160 | Val Acc: 86.65%  \n",
    "Epoch 05 | Train Loss: 0.2299 | Train Acc: 90.81% | Val Loss: 0.3180 | Val Acc: 86.54%  \n",
    "Epoch 06 | Train Loss: 0.2021 | Train Acc: 92.21% | Val Loss: 0.3329 | Val Acc: 86.25%  \n",
    "Epoch 07 | Train Loss: 0.1731 | Train Acc: 93.52% | Val Loss: 0.3721 | Val Acc: 86.06%  \n",
    "🛑 Early stopping triggered after 7 epochs.  \n",
    "  \n",
    "----------------------------------------  \n",
    "  - 최종 테스트 손실 (Loss): 0.3249  \n",
    "  - 최종 테스트 정확도 (Accuracy): 86.30%  \n",
    "----------------------------------------  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986e04a5-1c3b-4688-8eab-fc238eb15fe4",
   "metadata": {},
   "source": [
    "✅ KoNLPy(Mecab) 토크나이저 준비 완료 (vocab_size: 20000)  \n",
    "  \n",
    "Epoch 01 | Train Loss: 0.4907 | Train Acc: 73.94% | Val Loss: 0.3606 | Val Acc: 83.89%  \n",
    "Epoch 02 | Train Loss: 0.3332 | Train Acc: 85.63% | Val Loss: 0.3277 | Val Acc: 85.88%  \n",
    "Epoch 03 | Train Loss: 0.2883 | Train Acc: 87.91% | Val Loss: 0.3155 | Val Acc: 86.53%  \n",
    "Epoch 04 | Train Loss: 0.2553 | Train Acc: 89.72% | Val Loss: 0.3141 | Val Acc: 86.85%  \n",
    "Epoch 05 | Train Loss: 0.2262 | Train Acc: 91.04% | Val Loss: 0.3241 | Val Acc: 86.67%  \n",
    "Epoch 06 | Train Loss: 0.1961 | Train Acc: 92.58% | Val Loss: 0.3430 | Val Acc: 86.58%  \n",
    "Epoch 07 | Train Loss: 0.1669 | Train Acc: 93.93% | Val Loss: 0.3711 | Val Acc: 86.40%  \n",
    "🛑 Early stopping triggered after 7 epochs.  \n",
    "  \n",
    "----------------------------------------  \n",
    "  - 최종 테스트 손실 (Loss): 0.3227  \n",
    "  - 최종 테스트 정확도 (Accuracy): 86.20%  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecefb74-73c7-474e-9c4b-1cf4af5fdb5c",
   "metadata": {},
   "source": [
    "✅ KoNLPy(Mecab) 토크나이저 준비 완료 (vocab_size: 30000)  \n",
    "  \n",
    "Epoch 01 | Train Loss: 0.4783 | Train Acc: 75.43% | Val Loss: 0.3633 | Val Acc: 83.89%  \n",
    "Epoch 02 | Train Loss: 0.3358 | Train Acc: 85.54% | Val Loss: 0.3286 | Val Acc: 85.63%  \n",
    "Epoch 03 | Train Loss: 0.2889 | Train Acc: 88.06% | Val Loss: 0.3157 | Val Acc: 86.34%  \n",
    "Epoch 04 | Train Loss: 0.2536 | Train Acc: 89.77% | Val Loss: 0.3076 | Val Acc: 86.91%  \n",
    "Epoch 05 | Train Loss: 0.2219 | Train Acc: 91.33% | Val Loss: 0.3161 | Val Acc: 86.73%  \n",
    "Epoch 06 | Train Loss: 0.1912 | Train Acc: 92.86% | Val Loss: 0.3369 | Val Acc: 86.48%  \n",
    "Epoch 07 | Train Loss: 0.1620 | Train Acc: 94.16% | Val Loss: 0.3642 | Val Acc: 86.30%  \n",
    "🛑 Early stopping triggered after 7 epochs.  \n",
    "  \n",
    "----------------------------------------  \n",
    "  - 최종 테스트 손실 (Loss): 0.3170  \n",
    "  - 최종 테스트 정확도 (Accuracy): 86.44%  \n",
    "----------------------------------------  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83417c48-323e-433b-9668-143bf9bd35b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7c3165-d71a-48ee-8975-f972384c8fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 추가 전처리 테스트\n",
    " #불용어 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb34bebb-27ec-4a2b-b44a-59286e33c719",
   "metadata": {},
   "source": [
    "# ====================================================================================\n",
    "# 4. 데이터 준비 함수 (수정본)\n",
    "# ====================================================================================\n",
    "def load_and_preprocess_data():\n",
    "    \"\"\"NSMC 데이터를 로드하고 사용자 정의 전처리를 적용하는 함수\"\"\"\n",
    "    # 1. 데이터 경로를 설정하고 파일을 읽어옵니다.\n",
    "    data_path = os.path.join(os.getenv(\"HOME\"), 'work', 'workplace', 'AIFFEL_quest_rs', 'Exploration', 'Quest05', 'sentiment_classification', 'data')\n",
    "    train_data = pd.read_table(os.path.join(data_path, 'ratings_train.txt'))\n",
    "    test_data = pd.read_table(os.path.join(data_path, 'ratings_test.txt'))\n",
    "\n",
    "    # 2. --- 훈련 데이터 기본 정제 ---\n",
    "    train_data.dropna(subset=['document'], inplace=True) # 'document' 열에 결측치가 있는 행을 제거합니다.\n",
    "    train_data.drop_duplicates(subset=['document'], inplace=True) # 'document' 열의 내용이 중복되는 행을 제거합니다.\n",
    "\n",
    "    # 3. --- 테스트 데이터 기본 정제 ---\n",
    "    test_data.dropna(subset=['document'], inplace=True) # 'document' 열에 결측치가 있는 행을 제거합니다.\n",
    "    test_data.drop_duplicates(subset=['document'], inplace=True) # 'document' 열의 내용이 중복되는 행을 제거합니다.\n",
    "\n",
    "    # 4. --- 사용자 정의 불용어 전처리 (추가된 부분) ---\n",
    "    # 분석을 통해 도출한, 긍/부정 판단에 혼란을 줄 수 있는 공통 단어를 불용어로 정의합니다.\n",
    "    CUSTOM_STOPWORDS = [\n",
    "        '영화', '너무', '정말', '진짜', '그냥', '보고', '하는', '이', '그', '것', '또',\n",
    "        '더', '수', '좀', '잘', '다', '말', '안', '본', '뭐', '없는', '같다', '없다',\n",
    "        '봤는데', '연기', '배우', '점', '내', '난', '참', '왜', '다시', '같은', '완전',\n",
    "        '정도', '그래서', '그리고', '하지만', '근데', '일단', '오늘', '역시', '딱', '특히'\n",
    "    ]\n",
    "\n",
    "    def remove_stopwords(text):\n",
    "        \"\"\"입력된 텍스트에서 CUSTOM_STOPWORDS 리스트에 포함된 단어를 제거하는 함수\"\"\"\n",
    "        if not isinstance(text, str): # 만약 입력값이 문자열이 아니라면\n",
    "            return '' # 빈 문자열을 반환합니다.\n",
    "        words = text.split(' ') # 텍스트를 공백 기준으로 단어 리스트로 분리합니다.\n",
    "        filtered_words = [word for word in words if word not in CUSTOM_STOPWORDS] # 불용어에 포함되지 않는 단어만 선택합니다.\n",
    "        return ' '.join(filtered_words) # 필터링된 단어들을 다시 공백으로 연결하여 문장으로 만듭니다.\n",
    "\n",
    "    print(\"⏳ 사용자 정의 불용어 제거를 시작합니다...\")\n",
    "    # 훈련 데이터와 테스트 데이터의 'document' 열에 불용어 제거 함수를 적용합니다.\n",
    "    train_data['document'] = train_data['document'].apply(remove_stopwords)\n",
    "    test_data['document'] = test_data['document'].apply(remove_stopwords)\n",
    "    \n",
    "    # (선택) 불용어 제거 후 내용이 완전히 비어버린 행이 있다면 제거합니다.\n",
    "    train_data = train_data[train_data['document'] != '']\n",
    "    test_data = test_data[test_data['document'] != '']\n",
    "    print(\"✅ 사용자 정의 불용어 제거 완료.\")\n",
    "\n",
    "    # 5. --- 데이터셋 분리 ---\n",
    "    # 전처리된 훈련 데이터를 훈련용과 검증용으로 8:2 비율로 분리합니다.\n",
    "    train_set, val_set = train_test_split(train_data, test_size=0.2, random_state=CFG['SEED'], stratify=train_data['label'])\n",
    "\n",
    "    print(\"✅ 데이터 로드 및 모든 전처리 완료\")\n",
    "    return train_set, val_set, test_data # 최종적으로 분리된 데이터셋들을 반환합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47189659-796e-4a45-81f2-34c6ec25ded4",
   "metadata": {},
   "source": [
    "✅ 사용자 정의 불용어 제거 완료.  \n",
    "✅ SentencePiece 토크나이저 준비 완료 (vocab_size: 10000)  \n",
    "  \n",
    "Epoch 01 | Train Loss: 0.5448 | Train Acc: 69.99% | Val Loss: 0.4047 | Val Acc: 81.70%   \n",
    "Epoch 02 | Train Loss: 0.3523 | Train Acc: 84.70% | Val Loss: 0.3566 | Val Acc: 84.33%  \n",
    "Epoch 03 | Train Loss: 0.3049 | Train Acc: 87.07% | Val Loss: 0.3517 | Val Acc: 84.85%  \n",
    "Epoch 04 | Train Loss: 0.2768 | Train Acc: 88.52% | Val Loss: 0.3641 | Val Acc: 84.65%  \n",
    "Epoch 05 | Train Loss: 0.2467 | Train Acc: 89.85% | Val Loss: 0.3641 | Val Acc: 84.67%  \n",
    "Epoch 06 | Train Loss: 0.2134 | Train Acc: 91.52% | Val Loss: 0.3836 | Val Acc: 83.96%  \n",
    "🛑 Early stopping triggered after 6 epochs.  \n",
    "  \n",
    "----------------------------------------  \n",
    "  - 최종 테스트 손실 (Loss): 0.3505  \n",
    "  - 최종 테스트 정확도 (Accuracy): 84.92%  \n",
    "----------------------------------------  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbc1d1c-2342-4257-a5fc-15d9beba58c9",
   "metadata": {},
   "source": [
    "✅ 사용자 정의 불용어 제거 완료.  \n",
    "✅ KoNLPy(Mecab) 토크나이저 준비 완료 (vocab_size: 10000)  \n",
    "  \n",
    "Epoch 01 | Train Loss: 0.4803 | Train Acc: 75.03% | Val Loss: 0.3707 | Val Acc: 83.11%  \n",
    "Epoch 02 | Train Loss: 0.3366 | Train Acc: 85.24% | Val Loss: 0.3404 | Val Acc: 85.10%  \n",
    "Epoch 03 | Train Loss: 0.2942 | Train Acc: 87.46% | Val Loss: 0.3342 | Val Acc: 85.56%  \n",
    "Epoch 04 | Train Loss: 0.2658 | Train Acc: 88.98% | Val Loss: 0.3267 | Val Acc: 85.85%  \n",
    "Epoch 05 | Train Loss: 0.2397 | Train Acc: 90.23% | Val Loss: 0.3350 | Val Acc: 85.65%  \n",
    "Epoch 06 | Train Loss: 0.2128 | Train Acc: 91.53% | Val Loss: 0.3497 | Val Acc: 85.32%  \n",
    "Epoch 07 | Train Loss: 0.1854 | Train Acc: 92.92% | Val Loss: 0.3707 | Val Acc: 85.73%  \n",
    "🛑 Early stopping triggered after 7 epochs.  \n",
    "  \n",
    "----------------------------------------  \n",
    "  - 최종 테스트 손실 (Loss): 0.3267  \n",
    "  - 최종 테스트 정확도 (Accuracy): 85.86%  \n",
    "----------------------------------------  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d10c38-691a-46b6-a92d-8b3756075839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ba8512-476a-4faa-8fcf-d6f479dcdcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentencepiece 토크나이저 모델 변경 bpe > unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60623231-bcce-4f13-a1a9-3c9e0c4037c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# 2. 중앙 설정 (Configuration)\n",
    "# ====================================================================================\n",
    "CFG = {\n",
    "    # --- 실험 선택 ---\n",
    "    'TOKENIZER_TYPE': 'SentencePiece', # 'SentencePiece' 또는 'KoNLPy' 선택\n",
    "    'KONLPY_TOKENIZER': 'Mecab',     # TOKENIZER_TYPE이 'KoNLPy'일 경우, 사용할 형태소 분석기\n",
    "    \n",
    "    # --- SentencePiece 하이퍼파라미터 ---\n",
    "    'SP_VOCAB_SIZE': 10000,          # SentencePiece 단어 집합 크기\n",
    "    'SP_MODEL_TYPE': 'unigram',          # 'bpe' 또는 'unigram'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a6190d-2dbd-4742-b508-313220d182a7",
   "metadata": {},
   "source": [
    "✅ 'SP_MODEL_TYPE': 'unigram' 토크나이저 준비 완료 (vocab_size: 10000)  \n",
    "  \n",
    "Epoch 01 | Train Loss: 0.5444 | Train Acc: 69.63% | Val Loss: 0.3956 | Val Acc: 82.28%  \n",
    "Epoch 02 | Train Loss: 0.3510 | Train Acc: 84.78% | Val Loss: 0.3459 | Val Acc: 84.75%  \n",
    "Epoch 03 | Train Loss: 0.3039 | Train Acc: 87.08% | Val Loss: 0.3380 | Val Acc: 85.38%  \n",
    "Epoch 04 | Train Loss: 0.2771 | Train Acc: 88.41% | Val Loss: 0.3371 | Val Acc: 85.41%  \n",
    "Epoch 05 | Train Loss: 0.2501 | Train Acc: 89.77% | Val Loss: 0.3464 | Val Acc: 85.43%  \n",
    "Epoch 06 | Train Loss: 0.2195 | Train Acc: 91.24% | Val Loss: 0.3649 | Val Acc: 85.16%  \n",
    "Epoch 07 | Train Loss: 0.1855 | Train Acc: 92.84% | Val Loss: 0.4018 | Val Acc: 85.00%  \n",
    "🛑 Early stopping triggered after 7 epochs.  \n",
    "  \n",
    "----------------------------------------  \n",
    "  - 최종 테스트 손실 (Loss): 0.3421  \n",
    "  - 최종 테스트 정확도 (Accuracy): 85.19%  \n",
    "----------------------------------------  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e153f62d-304d-464d-8bff-83dd69931869",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93ba3d3-d877-439e-9775-5172ab80d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 옵션이 추가된 SentencePiece 토크나이저 실험\n",
    "    #character_coverage, user_defined_symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4735674-343c-490e-92ad-08d39eab7292",
   "metadata": {},
   "source": [
    "-character_coverage\t1.0\t'ㅋ', '!', '♡' 등 모든 기호와 이모티콘을 단어 사전에 포함시키기 위함입니다. \n",
    "기본값(0.9995)은 드물게 나타나는 문자를 제외할 수 있으므로 1.0으로 설정하여 모든 정보를 보존합니다.  \n",
    "   \n",
    "-user_defined_symbols\tㅋㅋ, ㅎㅎ, ㅠㅠ, ㅡㅡ, !, ?\t등 옵션에 자주 쓰이는 기호들을 등록하면, \n",
    "SentencePiece가 이들을 하나의 의미 있는 단위(토큰)로 취급할 수 있음. \n",
    "'ㅋㅋ'가 'ㅋ', 'ㅋ'로 분리되는 것을 방지하여 감정 신호를 그대로 유지할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437f97f5-83a7-49e9-ba9d-e7f97652e5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# 5. 토크나이저 생성 함수 (KoNLPy 로직 수정)\n",
    "# ====================================================================================\n",
    "# get_tokenizer_modified\n",
    "def get_tokenizer(cfg, train_df):\n",
    "    \"\"\"CFG에 따라 SentencePiece 또는 KoNLPy 토크나이저와 vocab_size를 반환\"\"\"\n",
    "\n",
    "    if cfg['TOKENIZER_TYPE'] == 'SentencePiece':\n",
    "        # --- (SentencePiece 로직 수정) ---\n",
    "        corpus_path = 'nsmc_corpus.txt' # 훈련 데이터로 사용할 텍스트 파일 경로입니다.\n",
    "        model_prefix = f'nsmc_{cfg[\"SP_MODEL_TYPE\"]}_{cfg[\"SP_VOCAB_SIZE\"]}' # 생성될 SentencePiece 모델 파일의 접두사입니다.\n",
    "        train_df['document'].to_csv(corpus_path, index=False, header=False) # 훈련 데이터를 텍스트 파일로 저장합니다.\n",
    "        \n",
    "        # <<<--- 수정된 부분 START ---\n",
    "        \n",
    "        # 1. user_defined_symbols 옵션에 사용할 기호들을 리스트로 정의합니다.\n",
    "        user_symbols = ['ㅋㅋ', 'ㅎㅎ', 'ㅠㅠ', 'ㅡㅡ', '!', '?']\n",
    "        # 2. 리스트를 쉼표(,)로 구분된 문자열 형태로 변환합니다. (ex: 'ㅋㅋ,ㅎㅎ,ㅠㅠ,...')\n",
    "        user_symbols_str = ','.join(user_symbols)\n",
    "\n",
    "        # 3. SentencePieceTrainer.train에 새로운 옵션을 추가합니다.\n",
    "        spm.SentencePieceTrainer.train(\n",
    "            f'--input={corpus_path} --model_prefix={model_prefix} '\n",
    "            f'--vocab_size={cfg[\"SP_VOCAB_SIZE\"]} --model_type={cfg[\"SP_MODEL_TYPE\"]} '\n",
    "            f'--character_coverage=1.0 ' # 모든 문자를 단어 사전에 포함시킵니다.\n",
    "            f'--user_defined_symbols={user_symbols_str}' # 사용자가 정의한 기호를 하나의 토큰으로 취급합니다.\n",
    "        )\n",
    "        # <<<--- 수정된 부분 END ---\n",
    "\n",
    "        processor = spm.SentencePieceProcessor() # 학습된 모델을 로드할 프로세서를 생성합니다.\n",
    "        processor.load(f'{model_prefix}.model') # 학습된 SentencePiece 모델을 로드합니다.\n",
    "        vocab_size = processor.get_piece_size() # 최종 단어 집합의 크기를 가져옵니다.\n",
    "\n",
    "        def tokenize_fn(corpus, max_len):\n",
    "            \"\"\"입력된 문장(corpus)을 토큰화하고 패딩하는 내부 함수\"\"\"\n",
    "            sequences = [] # 토큰화된 시퀀스를 저장할 리스트입니다.\n",
    "            for sentence in corpus: # 각 문장에 대해 반복합니다.\n",
    "                ids = processor.encode_as_ids(str(sentence)) # 문장을 정수 ID 시퀀스로 변환합니다.\n",
    "                # 문장의 길이를 max_len에 맞게 자르거나, 부족한 부분을 0으로 채웁니다(패딩).\n",
    "                ids = ids[:max_len] if len(ids) > max_len else ids + [0] * (max_len - len(ids))\n",
    "                sequences.append(ids) # 처리된 시퀀스를 리스트에 추가합니다.\n",
    "            return torch.tensor(sequences, dtype=torch.long) # 최종 결과를 PyTorch 텐서로 변환하여 반환합니다.\n",
    "            \n",
    "        print(f\"✅ SentencePiece 토크나이저 준비 완료 (vocab_size: {vocab_size})\")\n",
    "        return tokenize_fn, vocab_size # 토큰화 함수와 단어 집합 크기를 반환합니다.\n",
    "\n",
    "    elif cfg['TOKENIZER_TYPE'] == 'KoNLPy':\n",
    "        # --- (KoNLPy 로직은 이전과 동일) ---\n",
    "        if cfg['KONLPY_TOKENIZER'] == 'Mecab':\n",
    "            tokenizer = Mecab('/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ko-dic')\n",
    "        else:\n",
    "            from konlpy.tag import Okt\n",
    "            tokenizer = Okt()\n",
    "            \n",
    "        word_counts = {}\n",
    "        for sentence in tqdm(train_df['document'], desc=\"KoNLPy Freq. Counting\"):\n",
    "            tokens = tokenizer.morphs(str(sentence))\n",
    "            for token in tokens:\n",
    "                word_counts[token] = word_counts.get(token, 0) + 1\n",
    "        \n",
    "        sorted_words = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "        top_words = sorted_words[:cfg['KONLPY_VOCAB_SIZE'] - 2]\n",
    "        \n",
    "        word_index = {'<PAD>': 0, '<UNK>': 1}\n",
    "        for word in top_words:\n",
    "            word_index[word] = len(word_index)\n",
    "            \n",
    "        vocab_size = len(word_index)\n",
    "\n",
    "        def tokenize_fn(corpus, max_len):\n",
    "            sequences = []\n",
    "            for sentence in corpus:\n",
    "                tokens = tokenizer.morphs(str(sentence))\n",
    "                ids = [word_index.get(token, 1) for token in tokens] \n",
    "                ids = ids[:max_len] if len(ids) > max_len else ids + [0] * (max_len - len(ids))\n",
    "                sequences.append(ids)\n",
    "            return torch.tensor(sequences, dtype=torch.long)\n",
    "            \n",
    "        print(f\"✅ KoNLPy({cfg['KONLPY_TOKENIZER']}) 토크나이저 준비 완료 (vocab_size: {vocab_size})\")\n",
    "        return tokenize_fn, vocab_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c78fa1-26ba-4663-a747-2ef39f394562",
   "metadata": {},
   "source": [
    "✅ SentencePiece 토크나이저 준비 완료 (vocab_size: 10000)  \n",
    "     -character_coverage, user_defined_symbols 수정  \n",
    "       \n",
    "Epoch 01 | Train Loss: 0.5392 | Train Acc: 70.69% | Val Loss: 0.3970 | Val Acc: 82.17%  \n",
    "Epoch 02 | Train Loss: 0.3524 | Train Acc: 84.78% | Val Loss: 0.3490 | Val Acc: 84.84%  \n",
    "Epoch 03 | Train Loss: 0.3069 | Train Acc: 86.92% | Val Loss: 0.3369 | Val Acc: 85.31%  \n",
    "Epoch 04 | Train Loss: 0.2816 | Train Acc: 88.19% | Val Loss: 0.3335 | Val Acc: 85.55%  \n",
    "Epoch 05 | Train Loss: 0.2558 | Train Acc: 89.44% | Val Loss: 0.3449 | Val Acc: 85.40%  \n",
    "Epoch 06 | Train Loss: 0.2266 | Train Acc: 90.80% | Val Loss: 0.3592 | Val Acc: 85.11%  \n",
    "Epoch 07 | Train Loss: 0.1932 | Train Acc: 92.43% | Val Loss: 0.4015 | Val Acc: 84.96%  \n",
    "🛑 Early stopping triggered after 7 epochs.  \n",
    "  \n",
    "----------------------------------------  \n",
    "  - 최종 테스트 손실 (Loss): 0.3390  \n",
    "  - 최종 테스트 정확도 (Accuracy): 85.19%  \n",
    "----------------------------------------  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6470cdd0-dcf3-4689-b92d-8fe09ee7e577",
   "metadata": {},
   "source": [
    "최대 길이 40 -> 110  \n",
    "✅ SentencePiece 토크나이저 준비 완료 (vocab_size: 10000)  \n",
    "  \n",
    "Epoch 01 | Train Loss: 0.5775 | Train Acc: 67.12% | Val Loss: 0.4165 | Val Acc: 81.40%  \n",
    "Epoch 02 | Train Loss: 0.3623 | Train Acc: 84.48% | Val Loss: 0.3483 | Val Acc: 85.01%  \n",
    "Epoch 03 | Train Loss: 0.3029 | Train Acc: 87.26% | Val Loss: 0.3360 | Val Acc: 85.52%  \n",
    "Epoch 04 | Train Loss: 0.2720 | Train Acc: 88.86% | Val Loss: 0.3403 | Val Acc: 85.62%   \n",
    "Epoch 05 | Train Loss: 0.2409 | Train Acc: 90.31% | Val Loss: 0.3518 | Val Acc: 85.33%  \n",
    "Epoch 06 | Train Loss: 0.2068 | Train Acc: 91.84% | Val Loss: 0.3768 | Val Acc: 85.09%  \n",
    "🛑 Early stopping triggered after 6 epochs.  \n",
    "  \n",
    "----------------------------------------  \n",
    "  - 최종 테스트 손실 (Loss): 0.3420  \n",
    "  - 최종 테스트 정확도 (Accuracy): 85.29%  \n",
    "----------------------------------------  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1117ec27-51d5-4385-b1cd-dc3b433a6ec2",
   "metadata": {},
   "source": [
    "최대 길이 40 -> 110  \n",
    "✅ KoNLPy(Mecab) 토크나이저 준비 완료 (vocab_size: 10000)  \n",
    " \n",
    "Epoch 01 | Train Loss: 0.4902 | Train Acc: 74.49% | Val Loss: 0.3662 | Val Acc: 83.49%  \n",
    "Epoch 02 | Train Loss: 0.3320 | Train Acc: 85.73% | Val Loss: 0.3188 | Val Acc: 86.05%  \n",
    "Epoch 03 | Train Loss: 0.2858 | Train Acc: 88.03% | Val Loss: 0.3071 | Val Acc: 86.89%  \n",
    "Epoch 04 | Train Loss: 0.2563 | Train Acc: 89.50% | Val Loss: 0.3076 | Val Acc: 86.98%  \n",
    "Epoch 05 | Train Loss: 0.2298 | Train Acc: 90.77% | Val Loss: 0.3121 | Val Acc: 87.16%  \n",
    "Epoch 06 | Train Loss: 0.2022 | Train Acc: 91.95% | Val Loss: 0.3412 | Val Acc: 86.88%  \n",
    "🛑 Early stopping triggered after 6 epochs.  \n",
    "   \n",
    "----------------------------------------  \n",
    "  - 최종 테스트 손실 (Loss): 0.3159  \n",
    "  - 최종 테스트 정확도 (Accuracy): 86.37%  \n",
    "----------------------------------------  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5cf4db-0d97-45f1-96a5-62165a2c640a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71a5f77c-023d-40ad-9d5e-bbf836b52ffc",
   "metadata": {},
   "source": [
    "KoNLPy 품사(POS) 필터링이 적용된 토크나이저 생성-형용사 강조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7efa45-eef7-4583-8ccf-3f9046dc619c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# 5. 토크나이저 생성 함수 (KoNLPy 로직 수정)\n",
    "# ====================================================================================\n",
    "def get_tokenizer(cfg, train_df):\n",
    "    \"\"\"CFG에 따라 SentencePiece 또는 KoNLPy 토크나이저와 vocab_size를 반환\"\"\"\n",
    "    \n",
    "    if cfg['TOKENIZER_TYPE'] == 'SentencePiece':\n",
    "        # --- (SentencePiece 로직은 이전과 동일) ---\n",
    "        corpus_path = 'nsmc_corpus.txt'\n",
    "        model_prefix = f'nsmc_{cfg[\"SP_MODEL_TYPE\"]}_{cfg[\"SP_VOCAB_SIZE\"]}'\n",
    "        train_df['document'].to_csv(corpus_path, index=False, header=False)\n",
    "        spm.SentencePieceTrainer.train(\n",
    "            f'--input={corpus_path} --model_prefix={model_prefix} '\n",
    "            f'--vocab_size={cfg[\"SP_VOCAB_SIZE\"]} --model_type={cfg[\"SP_MODEL_TYPE\"]}'\n",
    "        )\n",
    "        processor = spm.SentencePieceProcessor()\n",
    "        processor.load(f'{model_prefix}.model')\n",
    "        vocab_size = processor.get_piece_size()\n",
    "\n",
    "        def tokenize_fn(corpus, max_len):\n",
    "            sequences = []\n",
    "            for sentence in corpus:\n",
    "                ids = processor.encode_as_ids(str(sentence))\n",
    "                ids = ids[:max_len] if len(ids) > max_len else ids + [0] * (max_len - len(ids))\n",
    "                sequences.append(ids)\n",
    "            return torch.tensor(sequences, dtype=torch.long)\n",
    "            \n",
    "        print(f\"✅ SentencePiece 토크나이저 준비 완료 (vocab_size: {vocab_size})\")\n",
    "        return tokenize_fn, vocab_size\n",
    "\n",
    "    elif cfg['TOKENIZER_TYPE'] == 'KoNLPy':\n",
    "        # --- KoNLPy 토크나이저 생성 (POS 필터링 로직 추가) ---\n",
    "        if cfg['KONLPY_TOKENIZER'] == 'Mecab':\n",
    "            tokenizer = Mecab('/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ko-dic')\n",
    "        else:\n",
    "            from konlpy.tag import Okt\n",
    "            tokenizer = Okt()\n",
    "\n",
    "        # <<<--- 수정된 부분 START ---\n",
    "\n",
    "        # 1. 어휘 사전에 포함할 품사(POS) 태그를 리스트로 정의합니다.\n",
    "        # Mecab 품사 태그 기준: NNG(일반 명사), NNP(고유 명사), VV(동사), VA(형용사)\n",
    "        TARGET_POS = ['NNG', 'NNP', 'VV', 'VA']\n",
    "        \n",
    "        # 2. 단어 빈도수 계산 시, 정의된 품사에 해당하는 단어만 카운트합니다.\n",
    "        word_counts = {}\n",
    "        print(\"⏳ KoNLPy 단어 빈도 계산 중 (POS 필터링 적용)...\")\n",
    "        for sentence in tqdm(train_df['document']):\n",
    "            # tokenizer.pos()를 사용하여 (단어, 품사) 튜플의 리스트를 반환받습니다.\n",
    "            pos_tokens = tokenizer.pos(str(sentence))\n",
    "            # TARGET_POS 리스트에 포함된 품사를 가진 단어(토큰)만 추출합니다.\n",
    "            tokens = [token for token, pos in pos_tokens if pos in TARGET_POS]\n",
    "            for token in tokens: # 필터링된 단어들에 대해서만 빈도수를 계산합니다.\n",
    "                word_counts[token] = word_counts.get(token, 0) + 1\n",
    "        \n",
    "        # 3. 빈도수 기준으로 상위 단어 선택 (기존과 동일)\n",
    "        sorted_words = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "        top_words = sorted_words[:cfg['KONLPY_VOCAB_SIZE'] - 2]\n",
    "        \n",
    "        # 4. 최종 단어 사전 구축 (기존과 동일)\n",
    "        word_index = {'<PAD>': 0, '<UNK>': 1}\n",
    "        for word in top_words:\n",
    "            word_index[word] = len(word_index)\n",
    "            \n",
    "        vocab_size = len(word_index)\n",
    "\n",
    "        def tokenize_fn(corpus, max_len):\n",
    "            \"\"\"입력된 문장을 토큰화하고 패딩하는 내부 함수 (POS 필터링 적용)\"\"\"\n",
    "            sequences = []\n",
    "            for sentence in corpus:\n",
    "                # 5. 토큰화 시에도 어휘 사전을 만들 때와 동일한 품사 필터링을 적용합니다.\n",
    "                pos_tokens = tokenizer.pos(str(sentence))\n",
    "                tokens = [token for token, pos in pos_tokens if pos in TARGET_POS]\n",
    "                ids = [word_index.get(token, 1) for token in tokens] # 사전에 없으면 <UNK> (1)\n",
    "                ids = ids[:max_len] if len(ids) > max_len else ids + [0] * (max_len - len(ids))\n",
    "                sequences.append(ids)\n",
    "            return torch.tensor(sequences, dtype=torch.long)\n",
    "        \n",
    "        # <<<--- 수정된 부분 END ---\n",
    "            \n",
    "        print(f\"✅ KoNLPy({cfg['KONLPY_TOKENIZER']}) 토크나이저 준비 완료 (vocab_size: {vocab_size})\")\n",
    "        return tokenize_fn, vocab_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b18ee86-24cc-4985-a10d-b9b3fcb111f1",
   "metadata": {},
   "source": [
    "✅ KoNLPy(Mecab) 토크나이저 준비 완료 (vocab_size: 31038)  \n",
    "  \n",
    "Epoch 01 | Train Loss: 0.5346 | Train Acc: 71.21% | Val Loss: 0.4610 | Val Acc: 77.38%  \n",
    "Epoch 02 | Train Loss: 0.4332 | Train Acc: 79.67% | Val Loss: 0.4375 | Val Acc: 79.18%  \n",
    "Epoch 03 | Train Loss: 0.3960 | Train Acc: 82.08% | Val Loss: 0.4308 | Val Acc: 79.62%  \n",
    "Epoch 04 | Train Loss: 0.3678 | Train Acc: 83.85% | Val Loss: 0.4304 | Val Acc: 79.76%  \n",
    "Epoch 05 | Train Loss: 0.3385 | Train Acc: 85.40% | Val Loss: 0.4433 | Val Acc: 79.69%  \n",
    "Epoch 06 | Train Loss: 0.3075 | Train Acc: 87.12% | Val Loss: 0.4724 | Val Acc: 79.01%  \n",
    "Epoch 07 | Train Loss: 0.2743 | Train Acc: 88.72% | Val Loss: 0.4935 | Val Acc: 78.64%  \n",
    "🛑 Early stopping triggered after 7 epochs.  \n",
    "  \n",
    "----------------------------------------  \n",
    "  - 최종 테스트 손실 (Loss): 0.4281   \n",
    "  - 최종 테스트 정확도 (Accuracy): 79.81%  \n",
    "----------------------------------------  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1d91f6-1d77-4c48-9bdd-a677cb5ec474",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
